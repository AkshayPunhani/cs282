{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - Using a VGGFase Classifier for speaker identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "from tensorflow.contrib import slim\n",
    "import cv2\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants for RESNet pre-processing and Pre-Processing Function\n",
    "batch_size = 5 \n",
    "num_epochs = 10\n",
    "image_height = 720\n",
    "image_width = 1280\n",
    "image_size = 224\n",
    "rnn_size = 256\n",
    "num_classes = 5\n",
    "learning_rate = 1e-3\n",
    "timestamps = 32\n",
    "\n",
    "_R_MEAN = 123.68\n",
    "_G_MEAN = 116.78\n",
    "_B_MEAN = 103.94\n",
    "\n",
    "def pre_process(images):\n",
    "  processed_images = []\n",
    "  for n in range(images.shape[0]):\n",
    "    image = images[n]\n",
    "    image = cv2.resize(image,  (int(224 * (image_width * 1.0 / image_height)) + 1 , 224))\n",
    "    image = image[:224, 87 : 87 + 224,:]\n",
    "    image = np.array(image, dtype=np.float32)\n",
    "    for i in range(3):\n",
    "      image[:,:,i] -= means[i]\n",
    "    processed_images.append(image)\n",
    "  processed_images = np.array(processed_images, dtype=np.float32)\n",
    "  return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Â Class that defines the ResNet Classifier (Using pre-trained weights from )\n",
    "\n",
    "class \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    images = tf.placeholder(dtype=tf.float32, name=\"images\", shape=[None, image_size, image_size, 3])\n",
    "    targets = tf.placeholder(dtype=tf.int32, name=\"targets\", shape=[None,])\n",
    "    with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=0.00001)):\n",
    "        pre_logits, _ = resnet_v2.resnet_v2_50(images,\n",
    "                                            num_classes=None,\n",
    "                                            is_training=True)\n",
    "\n",
    "    pre_logits = tf.stop_gradient(pre_logits)\n",
    "    variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=[\"resnet_v2_50/logits\", \"resnet_v2_50/AuxLogits\"])\n",
    "    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\"resnet_v2_50.ckpt\", variables_to_restore)\n",
    "\n",
    "    lm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size)])\n",
    "    # Use the dynamic_rnn function of Tensorflow to run the embedded inputs\n",
    "    # using the lm_cell you've created, and obtain the outputs of the RNN cell.\n",
    "    # You have created a cell, which represents a single block (column) of the RNN.\n",
    "    # dynamic_rnn will \"copy\" the cell for each element in your sequence, runs the input you provide through the cell,\n",
    "    # and returns the outputs and the states of the cell.\n",
    "    pre_logits = tf.reshape(pre_logits, (batch_size, timestamps, 2048))\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell = lm_cell, inputs = pre_logits, dtype = tf.float32)\n",
    "\n",
    "    # Use a dense layer to project the outputs of the RNN cell into the size of the\n",
    "    # vocabulary (vocab_size).\n",
    "    # output_logits should be of shape [None,input_length,vocab_size]\n",
    "    # You can look at the tf.layers.dense function\n",
    "    output_logits = tf.layers.dense(outputs[:,-1], units = num_classes)\n",
    "\n",
    "    # Setup the loss: using the sparse_softmax_cross_entropy.\n",
    "    # The logits are the output_logits we've computed.\n",
    "    # The targets are the gold labels we are trying to match\n",
    "    # Don't forget to use the targets_mask we have, so your loss is not off,\n",
    "    # And your model doesn't get rewarded for predicting PAD tokens\n",
    "    # You might have to cast the masks into float32. Look at the tf.cast function.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels = targets, logits = output_logits)\n",
    "    prediction = tf.cast(tf.argmax(output_logits, axis = -1), dtype=tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, targets), dtype=tf.float32))\n",
    "\n",
    "    # Setup an optimizer (SGD, RMSProp, Adam), you can find a list under tf.train.*\n",
    "    # And provide it with a start learning rate.\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    lr = tf.train.exponential_decay(learning_rate, global_step, 5000, 0.96, staircase = True)\n",
    "    #optimizer = tf.train.RMSPropOptimizer(lr)  \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # We create a train_op that requires the optimizer we've created to minimize the\n",
    "    # loss we've defined.\n",
    "    # look for the optimizer.minimize function, define what should be miniminzed.\n",
    "    # You can provide it with the provide an optional global_step parameter as well that keeps of how many\n",
    "    # Optimizations steps have been run.\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "names = {\"biden\": 0, \"hillary\":1, \"justin\":2, \"pelosi\":3, \"trump\":4}\n",
    "import glob\n",
    "def get_data_for_class(name):\n",
    "  print(\"Loading data for : \" , name)\n",
    "  files = glob.glob(\"/home/kratarth/Downloads/cs282/data/dataset/\" + name + \"/tf/*.tfrecords\")\n",
    "  print(files)\n",
    "  # Use only 1 file for now\n",
    "  num, videos = get_number_of_records(files[:1], 32)\n",
    "  return num, videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
